.. trust-ml documentation master file, created by
   sphinx-quickstart on Thu Jun  1 22:48:40 2023.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Trust-ML
========

Trust-ML is a machine learning-driven adversarial data generator that introduces naturally occurring distortions to the
original dataset to generate an adversarial subset. This framework provides a custom mix of distortions for evaluating
robustness of image classification models against both true negatives and false positives. Our framework enables users
to audit their algorithms with customizable distortions. With the help of RLAB, we can generate more effective and
efficient adversarial samples than other distortion-based benchmarks. Read below for further details on how RLAB works.
In our experiments, we demonstrate that **samples generated with our framework cause greater accuracy degradation of
state-of-the-art adversarial robustness methods (as tracked by** `RobustBench <https://robustbench.github.io/>`_) **than
ImageNet-C and CIFAR-10-C**. Examples of images generated by our framework are shown below.


.. figure:: images/imagenet-examples-blur.png
   :scale: 70 %
   :alt: A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet
   :align: center

   A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet. The images
   shown are for severity level 5 of the Gaussian blur distortion. For the same severity level, images from ours retain
   much more clarity while being more challenging to classify.


.. figure:: images/imagenet-examples-noise.png
   :scale: 70 %
   :alt: A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet
   :align: center

   A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet. The images
   shown are for severity level 5 of the Gaussian noise distortion. For the same severity level, images from ours retain
   much more clarity while being more challenging to classify.

Features
--------

* Generate adversarial samples for any type of distortion (see examples in figure below)
* Generate samples at multiple distortion levels (severity/difficulty levels)
* Robustness can be audited for both true negatives and false positives
* As RLAB only requires black-box access, any model (or ensemble) can be used as the victim in the generator


.. figure:: images/distortion-types.png
   :scale: 70 %
   :alt: Some examples of distortions that are supported by Trust-ML
   :align: center

   Some examples of distortions that are supported by Trust-ML


Getting Started
---------------

1. Head to the :doc:`installation` page to get set up with our software
2. Head to the :doc:`usage` page to get familiar with generating your own distorted data
3. You can evaluate on our distorted versions of ImageNet and CIFAR-10 used in the paper from `Zenodo <https://zenodo.org/record/8034833>`_ (see :doc:`usage` for dataset structure)


How RLAB Works
--------------

.. figure:: images/image-1.png
   :scale: 30 %
   :alt: Difference between RLAB and competitors
   :align: center

   Difference between RLAB and competitors

Unlike the existing traditional adversarial training approaches that use hand crafted attack strategy to generate adversarial samples, RLAB learns an attack strategy to generate more efficient adversarial samples.

.. figure:: images/image-2.png
   :scale: 10 %
   :alt: RLAB Workflow
   :align: center

   RLAB Workflow

The above figure shows the overall flow of the proposed method.  Given a data sample, RLAB divides the input into a set of grid and performs sensitivity analysis.  The agent performs two actions, one to find the patch to which distortions can be added and the patch from which earlier added distortion can be removed.  This is performed iteratively until the model misclassifies the given data sample.  The final sample with the perturbations in it is called an adversarial sample that contains information about the vulnerability of the model.

.. figure:: images/Visual-Comparison.jpg
   :scale: 10 %
   :alt: RLAB's distortion comparison with Patch Attack and Square Attack from the literature
   :align: center

   RLAB's distortion comparison with Patch Attack and Square Attack from the literature


Links
-----

* We host the source code for our benchmark on `GitHub <https://github.com/HewlettPackard/trust-ml/>`_.
* We provide our distorted versions of the ImageNet and CIFAR-10 datasets for download on `Zenodo <https://zenodo.org/record/8034833>`_.

.. toctree::
   :hidden:

   Home <self>
   installation
   usage
