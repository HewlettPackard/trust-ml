{% extends "!layout.html" %}
{%- block scripts %}
{{ super }}
<script type="application/ld+json">
{
  "layout": "index-template",
  "schemadotorg": {
    "@context": "http://schema.org/",
    "@type": "Dataset",
    "name": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "about": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "description": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "abstract": "We present a novel framework for generating adversarial benchmarks to evaluate the robustness of image classification models. The RLAB framework allows users to customize the types of distortions to be optimally applied to images, which helps address the specific distortions relevant to their deployment. The benchmark can generate datasets at various distortion levels to assess the robustness of different image classifiers. Our results show that the adversarial samples generated by our framework with any of the image classification models, like ResNet-50, Inception-V3, and VGG-16, are effective and transferable to other models causing them to fail. These failures happen even when these models are adversarially retrained using state-of-the-art techniques, demonstrating the generalizability of our adversarial samples. Our framework also allows the creation of adversarial samples for non-ground truth classes at different levels of intensity, enabling tunable benchmarks for the evaluation of false positives. We achieve competitive performance in terms of net $L_2$ distortion compared to state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we demonstrate our framework achieves such results with simple distortions like Gaussian noise without introducing unnatural artifacts or color bleeds. This is made possible by a model-based reinforcement learning (RL) agent and a technique that reduces a deep tree search of the image for model sensitivity to perturbations, to a one-level analysis and action. The flexibility of choosing distortions and setting classification probability thresholds for multiple classes makes our framework suitable for algorithmic audits.",
    "author": [
      "Soumyendu Sarkar",
      "Ashwin Ramesh Babu",
      "Sajad Mousavi",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Zachariah Carmichael",
      "Antonio Guillen",
      "Ricardo Luna Gutierrez",
      "Avisek Naug"
    ],
    "contributor": [
      {
        "@type": "Person",
        "name": "Lekhapriya Dheeraj Kashyap"
      }
    ],
    "keywords": [
      "trustworthiness",
      "robustness",
      "AI",
      "deep learning"
    ],
    "license": "MIT",
    "version": 1,
    "distribution": "https://hewlettpackard.github.io/trust-ml/",
    "copyrightHolder": [
      {
        "@type": "Organization",
        "name": "HPE"
      }
    ],
    "copyrightYear": 2023,
    "isAccessibleForFree": true,
    "maintainer": [
      {
        "@type": "Organization",
        "name": "HPE"
      }
    ],
    "url": "https://hewlettpackard.github.io/trust-ml/"
  }
}
</script>
{% endblock %}