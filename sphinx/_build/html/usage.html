<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Usage &mdash; Trust-ML 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
<jinja2.runtime.BlockReference object at 0x7f9e65febaf0>
<script type="application/ld+json">
{
  "layout": "index-template",
  "schemadotorg": {
    "@context": "http://schema.org/",
    "@type": "Dataset",
    "name": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "about": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "description": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "abstract": "We present a novel framework for generating adversarial benchmarks to evaluate the robustness of image classification models. The RLAB framework allows users to customize the types of distortions to be optimally applied to images, which helps address the specific distortions relevant to their deployment. The benchmark can generate datasets at various distortion levels to assess the robustness of different image classifiers. Our results show that the adversarial samples generated by our framework with any of the image classification models, like ResNet-50, Inception-V3, and VGG-16, are effective and transferable to other models causing them to fail. These failures happen even when these models are adversarially retrained using state-of-the-art techniques, demonstrating the generalizability of our adversarial samples. Our framework also allows the creation of adversarial samples for non-ground truth classes at different levels of intensity, enabling tunable benchmarks for the evaluation of false positives. We achieve competitive performance in terms of net $L_2$ distortion compared to state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we demonstrate our framework achieves such results with simple distortions like Gaussian noise without introducing unnatural artifacts or color bleeds. This is made possible by a model-based reinforcement learning (RL) agent and a technique that reduces a deep tree search of the image for model sensitivity to perturbations, to a one-level analysis and action. The flexibility of choosing distortions and setting classification probability thresholds for multiple classes makes our framework suitable for algorithmic audits.",
    "author": [
      "Soumyendu Sarkar",
      "Ashwin Ramesh Babu",
      "Sajad Mousavi",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Zachariah Carmichael",
      "Antonio Guillen",
      "Ricardo Luna Gutierrez",
      "Avisek Naug"
    ],
    "contributor": [
      {
        "@type": "Person",
        "name": "Lekhapriya Dheeraj Kashyap"
      }
    ],
    "keywords": [
      "trustworthiness",
      "robustness",
      "AI",
      "deep learning"
    ],
    "license": "MIT",
    "version": 1,
    "distribution": "https://hewlettpackard.github.io/trust-ml/",
    "copyrightHolder": [
      {
        "@type": "Organization",
        "name": "HPE"
      }
    ],
    "copyrightYear": 2023,
    "isAccessibleForFree": true,
    "maintainer": [
      {
        "@type": "Organization",
        "name": "HPE"
      }
    ],
    "url": "https://hewlettpackard.github.io/trust-ml/"
  }
}
</script>

    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Trust-ML
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="rlab.html">How RLAB Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#run">Run</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dataset-structure">Dataset Structure</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Trust-ML</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Usage</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/usage.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="usage">
<h1>Usage<a class="headerlink" href="#usage" title="Permalink to this heading"></a></h1>
<section id="run">
<h2>Run<a class="headerlink" href="#run" title="Permalink to this heading"></a></h2>
<p>The framework can be run through <code class="code docutils literal notranslate"><span class="pre">main.py</span></code>.
Example for having ResNet-50 as the victim model with which the adversarial samples can be generated:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main.py<span class="w"> </span>--model_path<span class="w"> </span>./models/imagenet/architecture/resnet50_imagenet1000.pk<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--weight_path<span class="w"> </span>./models/imagenet/weights/resnet50_imagenet1000.pt<span class="w"> </span>--dataset<span class="w"> </span>imagenet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_path<span class="w"> </span>./datasets/imagenet/val<span class="w"> </span>--log_dir<span class="w"> </span>./results/imagenet/resnet50
</pre></div>
</div>
<p>The related dataset is not included in the repository. Import it to the dataset path that you specify.</p>
<p><strong>CLI arguments:</strong></p>
<ul class="simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">model_path</span></code>: path to a trained model</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">weight_path</span></code>: path to the weights of the trained model</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">dataset</span></code>: dataset name. <em>If missing, add the configuration for your own dataset in dataset.py (currently configured: cifar10, caltech101, imagenet)</em></p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">dataset_path</span></code>: path to a dataset</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">log_dir</span></code>: directory to log to</p></li>
</ul>
</section>
<section id="dataset-structure">
<h2>Dataset Structure<a class="headerlink" href="#dataset-structure" title="Permalink to this heading"></a></h2>
<p>Datasets generated with Trust-ML are provided as pickle files. For instance,
see our data provided on <a class="reference external" href="https://zenodo.org/record/8034833">Zenodo</a>.
This data should be loaded through Python’s <code class="code docutils literal notranslate"><span class="pre">pickle</span></code> module. The
structure is a list as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Label</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Sample</span> <span class="n">ID</span> <span class="mi">1</span>
<span class="n">Image</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Label</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Sample</span> <span class="n">ID</span> <span class="mi">2</span>
<span class="o">...</span>
<span class="n">Image</span> <span class="n">N</span><span class="p">,</span> <span class="n">Label</span> <span class="n">N</span><span class="p">,</span> <span class="n">Sample</span> <span class="n">ID</span> <span class="n">N</span>
</pre></div>
</div>
<p>Each image is a NumPy array of shape <code class="code docutils literal notranslate"><span class="pre">H</span> <span class="pre">x</span> <span class="pre">W</span> <span class="pre">x</span> <span class="pre">3</span></code>, each label is an
integer-coded class label, and each sample ID has a correspondence with a
sample in the original dataset.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, HPE.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>