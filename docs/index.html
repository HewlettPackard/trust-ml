<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Trust-ML &mdash; Trust-ML 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
      <link rel="stylesheet" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
<jinja2.runtime.BlockReference object at 0x7fe14d7a53d0>
<script type="application/ld+json">
{
  "layout": "index-template",
  "schemadotorg": {
    "@context": "http://schema.org/",
    "@type": "Dataset",
    "name": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "about": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "description": "Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness",
    "abstract": "We present a novel framework for generating adversarial benchmarks to evaluate the robustness of image classification models. The RLAB framework allows users to customize the types of distortions to be optimally applied to images, which helps address the specific distortions relevant to their deployment. The benchmark can generate datasets at various distortion levels to assess the robustness of different image classifiers. Our results show that the adversarial samples generated by our framework with any of the image classification models, like ResNet-50, Inception-V3, and VGG-16, are effective and transferable to other models causing them to fail. These failures happen even when these models are adversarially retrained using state-of-the-art techniques, demonstrating the generalizability of our adversarial samples. Our framework also allows the creation of adversarial samples for non-ground truth classes at different levels of intensity, enabling tunable benchmarks for the evaluation of false positives. We achieve competitive performance in terms of net $L_2$ distortion compared to state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we demonstrate our framework achieves such results with simple distortions like Gaussian noise without introducing unnatural artifacts or color bleeds. This is made possible by a model-based reinforcement learning (RL) agent and a technique that reduces a deep tree search of the image for model sensitivity to perturbations, to a one-level analysis and action. The flexibility of choosing distortions and setting classification probability thresholds for multiple classes makes our framework suitable for algorithmic audits.",
    "author": [
      "Soumyendu Sarkar",
      "Ashwin Ramesh Babu",
      "Sajad Mousavi",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Zachariah Carmichael",
      "Antonio Guillen",
      "Ricardo Luna Gutierrez",
      "Avisek Naug"
    ],
    "contributor": [
      {
        "@type": "Person",
        "name": "Lekhapriya Dheeraj Kashyap"
      }
    ],
    "keywords": [
      "trustworthiness",
      "robustness",
      "AI",
      "deep learning"
    ],
    "license": "MIT",
    "version": 1,
    "distribution": "https://hewlettpackard.github.io/trust-ml/",
    "copyrightHolder": [
      {
        "@type": "Organization",
        "name": "HPE"
      }
    ],
    "copyrightYear": 2023,
    "isAccessibleForFree": true,
    "maintainer": [
      {
        "@type": "Organization",
        "name": "HPE"
      }
    ],
    "url": "https://hewlettpackard.github.io/trust-ml/"
  }
}
</script>

    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            Trust-ML
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Trust-ML</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Trust-ML</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="trust-ml">
<h1>Trust-ML<a class="headerlink" href="#trust-ml" title="Permalink to this heading"></a></h1>
<p>Trust-ML is a machine learning-driven adversarial data generator that introduces naturally occurring distortions to the
original dataset to generate an adversarial subset. This framework provides a custom mix of distortions for evaluating
robustness of image classification models against both true negatives and false positives. Our framework enables users
to audit their algorithms with customizable distortions. With the help of RLAB, we can generate more effective and
efficient adversarial samples than other distortion-based benchmarks. Read below for further details on how RLAB works.
In our experiments, we demonstrate that <strong>samples generated with our framework cause greater accuracy degradation of
state-of-the-art adversarial robustness methods (as tracked by</strong> <a class="reference external" href="https://robustbench.github.io/">RobustBench</a>) <strong>than
ImageNet-C and CIFAR-10-C</strong>. Examples of images generated by our framework are shown below.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/imagenet-examples-blur.png"><img alt="A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet" src="_images/imagenet-examples-blur.png" style="width: 685.3px; height: 373.09999999999997px;" /></a>
<figcaption>
<p><span class="caption-text">A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet. The images
shown are for severity level 5 of the Gaussian blur distortion. For the same severity level, images from ours retain
much more clarity while being more challenging to classify.</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/imagenet-examples-noise.png"><img alt="A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet" src="_images/imagenet-examples-noise.png" style="width: 685.3px; height: 373.09999999999997px;" /></a>
<figcaption>
<p><span class="caption-text">A subset of images from each of original ImageNet, ImageNet-C, and our distorted version of ImageNet. The images
shown are for severity level 5 of the Gaussian noise distortion. For the same severity level, images from ours retain
much more clarity while being more challenging to classify.</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<section id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>Generate adversarial samples for any type of distortion (see examples in figure below)</p></li>
<li><p>Generate samples at multiple distortion levels (severity/difficulty levels)</p></li>
<li><p>Robustness can be audited for both true negatives and false positives</p></li>
<li><p>As RLAB only requires black-box access, any model (or ensemble) can be used as the victim in the generator</p></li>
</ul>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="images/distortion-types.png"><img alt="Some examples of distortions that are supported by Trust-ML" src="images/distortion-types.png" /></a>
<figcaption>
<p><span class="caption-text">Some examples of distortions that are supported by Trust-ML</span><a class="headerlink" href="#id4" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Head to the <a class="reference internal" href="installation.html"><span class="doc">Installation</span></a> page to get set up with our software</p></li>
<li><p>Head to the <a class="reference internal" href="usage.html"><span class="doc">Usage</span></a> page to get familiar with generating your own distorted data</p></li>
<li><p>You can evaluate on our distorted versions of ImageNet and CIFAR-10 used in the paper from <a class="reference external" href="https://zenodo.org/record/8034833">Zenodo</a> (see <a class="reference internal" href="usage.html"><span class="doc">Usage</span></a> for dataset structure)</p></li>
</ol>
</section>
<section id="how-rlab-works">
<h2>How RLAB Works<a class="headerlink" href="#how-rlab-works" title="Permalink to this heading"></a></h2>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="_images/image-1.png"><img alt="Difference between RLAB and competitors" src="_images/image-1.png" style="width: 542.6999999999999px; height: 225.0px;" /></a>
<figcaption>
<p><span class="caption-text">Difference between RLAB and competitors</span><a class="headerlink" href="#id5" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Unlike the existing traditional adversarial training approaches that use hand crafted attack strategy to generate adversarial samples, RLAB learns an attack strategy to generate more efficient adversarial samples.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="_images/image-2.png"><img alt="RLAB Workflow" src="_images/image-2.png" style="width: 730.6px; height: 252.60000000000002px;" /></a>
<figcaption>
<p><span class="caption-text">RLAB Workflow</span><a class="headerlink" href="#id6" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>The above figure shows the overall flow of the proposed method.  Given a data sample, RLAB divides the input into a set of grid and performs sensitivity analysis.  The agent performs two actions, one to find the patch to which distortions can be added and the patch from which earlier added distortion can be removed.  This is performed iteratively until the model misclassifies the given data sample.  The final sample with the perturbations in it is called an adversarial sample that contains information about the vulnerability of the model.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="_images/Visual-Comparison.jpg"><img alt="RLAB's distortion comparison with Patch Attack and Square Attack from the literature" src="_images/Visual-Comparison.jpg" style="width: 583.5px; height: 344.8px;" /></a>
<figcaption>
<p><span class="caption-text">RLAB’s distortion comparison with Patch Attack and Square Attack from the literature</span><a class="headerlink" href="#id7" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="links">
<h2>Links<a class="headerlink" href="#links" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>We host the source code for our benchmark on <a class="reference external" href="https://github.com/HewlettPackard/trust-ml/">GitHub</a>.</p></li>
<li><p>We provide our distorted versions of the ImageNet and CIFAR-10 datasets for download on <a class="reference external" href="https://zenodo.org/record/8034833">Zenodo</a>.</p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, HPE.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>